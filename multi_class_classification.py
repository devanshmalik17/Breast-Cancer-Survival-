# -*- coding: utf-8 -*-
"""Multi-Class Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_sfuLBkypV41Eva0-IJM1gfI8OcqvTc
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE  # imblearn library can be installed using pip install imblearn
from imblearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Importing dataset and examining it
dataset = pd.read_csv("Beverage.csv")
pd.set_option('display.max_columns', None) # to make sure you can see all the columns in output window
print(dataset.head())
print(dataset.shape)
print(dataset.info())
print(dataset.describe())

# Converting Categorical features into Numerical features
dataset['quality'] = dataset['quality'].map({'Excellent':2, 'Normal':1, 'Poor':0})
print(dataset.info())

# Dividing dataset into label and feature sets
X = dataset.drop('quality', axis = 1) # Features
Y = dataset['quality'] # Labels
print(type(X))
print(type(Y))
print(X.shape)
print(Y.shape)

# Normalizing numerical features so that each feature has mean 0 and variance 1
feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(X)

# Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),
        ('classification', RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1))])
grid_param = {'classification__n_estimators': [400,450,500,550,600,650]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision_weighted', cv=5)

gd_sr.fit(X_scaled, Y)

best_parameters = gd_sr.best_params_
print("Optimal parameters:\n", best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print("Best mean cross-validated score:\n", best_result)


# Building random forest using the tuned parameter
rfc = RandomForestClassifier(n_estimators=600, criterion='entropy', max_features='auto', random_state=1)
rfc.fit(X_scaled,Y)
featimp = pd.Series(rfc.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp)

# Selecting features with higher sifnificance and redefining feature set
X_ = dataset[['alcohol', 'density', 'free sulfur dioxide', 'volatile acidity', 'total sulfur dioxide', 'residual sugar']]

feature_scaler = StandardScaler()
X_scaled_ = feature_scaler.fit_transform(X_)

# Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),
        ('classification', RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1))])
grid_param = {'classification__n_estimators': [400,450,500,550,600,650]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision_weighted', cv=5)

gd_sr.fit(X_scaled_, Y)

best_parameters = gd_sr.best_params_
print("Optimal parameters:\n", best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print("Best mean cross-validated score:\n", best_result)

################################################################################
# Implementing One-vs-Rest SVC
# Tuning SVC parameters 'kernel' and 'C';and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),
        ('classification',SVC(random_state=1))])
grid_param = {'classification__kernel': ['rbf','linear', 'poly', 'sigmoid'], 'classification__C':[1,10,100]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='precision_weighted', cv=5)

gd_sr.fit(X_scaled, Y)

best_parameters = gd_sr.best_params_
print("Optimal parameters:\n", best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print("Best mean cross-validated score:\n", best_result)