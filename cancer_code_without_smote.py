# -*- coding: utf-8 -*-
"""Cancer_Code_Without_SMOTE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Wu_YpBV4X4UNXtChWLEWF6P8LmIjEdW
"""



from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import tree
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE 
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from imblearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.linear_model import SGDClassifier
import joblib
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE  
from sklearn.neural_network import MLPClassifier
from imblearn.pipeline import Pipeline
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import classification_report



# Importing Data And Examining The Data 
data = pd.read_csv("/content/drive/MyDrive/SEER.csv")
pd.set_option('display.max_columns', None)
print(data.head())
print(data.shape)
print(data.info())
print(data.describe())


# Mapping Categorical Columns To Numbers Where They Can Be Mapped In A Mathematical Order 
data['T Stage '] = data['T Stage '].map({'T1':0, 'T2':1,'T3':2,'T4':3})
data['N Stage'] = data['N Stage'].map({'N1':0, 'N2':1,'N3':2})
data['6th Stage'] = data['6th Stage'].map({'IIA':0, 'IIB':1,'IIIA':2,'IIIB':3,'IIIC':4})
data['Grade'] = data['Grade'].map({'Well differentiated; Grade I':0, 'Moderately differentiated; Grade II':1,'Poorly differentiated; Grade III':2,'Undifferentiated; anaplastic; Grade IV':3})
data['A Stage'] = data['A Stage'].map({'Regional':0, 'Distant':1})
data['Estrogen Status'] = data['Estrogen Status'].map({'Positive':0, 'Negative':1})
data['Progesterone Status'] = data['Progesterone Status'].map({'Positive':0, 'Negative':1})
data['Status'] = data['Status'].map({'Alive':1, 'Dead':0})
print(data.info())


# Defining Target Variables and Independent Variable
x = data.drop(['Status','Race ','Marital Status'], axis = 1) # Independent Variables
y = data['Status'] # Target Variable 
  
# Normalizing The Feature Set 
scaler = StandardScaler()
scaledX = scaler.fit_transform(x)


# Dividing Dataset Into Training And Test Sets (Test Size = 30% Of The Entire Data)
X_train, X_test, Y_train, Y_test = train_test_split( scaledX, y, test_size = 0.3, random_state = 100)


##### Multi-Layer Perceptron ##### 
print('Multi-Layer Perceptron')
mlp = MLPClassifier(hidden_layer_sizes = (100,), random_state=1) ### Defining The Model and Hidden Layers 
grid = {'activation': ['logistic', 'tanh', 'relu'], 'learning_rate_init': [.001,.01,.1,1,10,100], 'max_iter' : [10,20,50,100,500,1000]} ## Specifying a grid of hyper-parameters
gridsearch = GridSearchCV(estimator=mlp, param_grid=grid, scoring='precision', cv=5) ## Passing the Model to gridsearchCV and Running Cross-Validation 
gridsearch.fit(X_train, Y_train) ## Passing the training data 
OptimalHyperparameters = gridsearch.best_params_ ## Getting the best hyper-parameter values 
Mean_CV_Score = gridsearch.best_score_ # Mean cross-validated score 
Best_MLP_Model = gridsearch.best_estimator_ ## Getting the best MLP model
print("Best Hyper Parameter Values For MLP Are: ",OptimalHyperparameters)
print("Best Precision Score By MLP Is: ",Mean_CV_Score)
print("Best Validated MLP Model Is: ",Best_MLP_Model)

print()
print()


###### Support Vector classifier ###############
print("###################################")
print("Support Vector Classifier")
svc = SVC(random_state=1) ## Defining The Model 
grid = {'kernel': ['linear','poly','rbf','sigmoid'], 'C': [.001,.01,.1,1,10,100]} ## Specifying a Grid Of Hyper-Parameters
gridsearch = GridSearchCV(estimator=svc, param_grid=grid, scoring='precision', cv=5)  ## Passing the Model to gridsearchCV and Running Cross-Validation 
gridsearch.fit(X_train, Y_train) ## Passing the Training Data 
OptimalHyperparameters = gridsearch.best_params_  ## Getting The Best Hyper-Parameter Values
Mean_CV_Score = gridsearch.best_score_ ## Mean Cross-Validated Score 
Best_SVC_Model = gridsearch.best_estimator_ 
print("Best Hyper Parameter Values For SVC Are: ",OptimalHyperparameters)
print("Best Precision Score By SVC Is: ",Mean_CV_Score)
print("The Validated SVC Model Is: ",Best_SVC_Model)

print()
print()



####### Decision Tree Classifier ##########
print("###############################")
print("Decision Tree Classifier")
dt= tree.DecisionTreeClassifier(criterion = 'entropy') ## Defining the model
grid =  {'max_depth': [1,2,3,5,10,15,20,25,30,50,70,100,120,150]} ## Specifying a Grid of Hyper-Parameters
gridsearch = GridSearchCV(estimator=dt, param_grid=grid, scoring='precision', cv=5) ## Passing the Model to gridsearchCV and Running Cross-Validation
gridsearch.fit(X_train, Y_train) ## Passing the Training Data 
OptimalHyperparameters = gridsearch.best_params_  ## Getting The Best Hyper-Parameter values
Mean_CV_Score = gridsearch.best_score_ ## Mean Cross Validated Score
Best_DecisionTree_Model = gridsearch.best_estimator_  ## Getting the Best Decision Tree Model 
print("Best Hyper Parameter Values For Decision Tree Is: ",OptimalHyperparameters)
print("Best Precision Score By Decision Tree Is: ",Mean_CV_Score)
print("The Validated Decision Tree Model Is: ",Best_DecisionTree_Model)

print()
print()


####### AdaBoost #############
print("###############################")
print("Adaboost Classifier")
adb = AdaBoostClassifier(random_state=1) ## Defining the model 
grid =  {'n_estimators': [1,2,3,5,10,15,20,25,30,50,70,100,120,150]} ## Specifying a Grid Of Hyper-Parameters 
gridsearch = GridSearchCV(estimator=adb, param_grid=grid, scoring='precision', cv=5) ## Passing the model to gridsearchCV and running cross-validation
gridsearch.fit(X_train, Y_train) ## passing the training data 
OptimalHyperparameters = gridsearch.best_params_ ## Getting the best hyper-parameter values
Mean_CV_Score = gridsearch.best_score_ # Mean cross-validated score of the best_estimator
Best_Adaboost_Model = gridsearch.best_estimator_ ## Getting the best Adaboost model 
print("Best Hyper Parameter Value For Adaboost Is: ",OptimalHyperparameters)
print("Best Precision Score By AdaBoost Is: ",Mean_CV_Score)
print("Best Validated AdaBoost Model Is: ",Best_Adaboost_Model)

print()
print()



######## Logistic Regression #########
print("##################################")
print("Logistic Regression")
lr = SGDClassifier(loss = 'log', penalty = 'elasticnet', random_state = 1) # Defining the Model
grid = {'eta0': [.001,.01,.1,1,10,100], 'max_iter' : [10,50,100,500,1000], 'alpha': [.001, .01,.1, 1,10,100], 'l1_ratio': [0,0.3,0.5,0.7,1]} ## Specifying a grid of hyper-parameters 
gridsearch = GridSearchCV(estimator=lr, param_grid=grid, scoring='precision', cv=5) ##  Passing the Model to GridsearchCV and running cross-validation passing the model to gridsearchCV
gridsearch.fit(X_train, Y_train) ## Passing the training data to gridsearch
OptimalHyperparameters = gridsearch.best_params_ ## Getting the best hyper-parameter values
Mean_CV_Score = gridsearch.best_score_ # Mean cross-validated score of the best_estimator
Best_LR_Model = gridsearch.best_estimator_ ## Getting the best LR model 
print("Best Hyper Parameter Values For Logistic Regression Are: ",OptimalHyperparameters)
print("Best Precision Score By Logistic Regression Is: ",Mean_CV_Score)
print("Best Validated Logistic Regression Model Is: ",Best_LR_Model)

print()
print()

## Random Forest Classifier ##########
print("###################################")
print("Random Forest Classifier")
rfc = RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1) ## Defining the Model 
grid = {'n_estimators': [1,2,3,4,5,6,10,20,30,40,50,100]} ## Specifying a grid of hyper-parameters 
gridsearch = GridSearchCV(estimator=rfc, param_grid=grid, scoring='precision', cv=5) ##  Passing the Model to GridsearchCV and running cross-validation passing the model to gridsearchCV
gridsearch.fit(X_train, Y_train) ## Passing the Training Data 
OptimalHyperparameters = gridsearch.best_params_
MeanCVScore = gridsearch.best_score_ # Mean cross-validated score
Best_RFC_Model = gridsearch.best_estimator_ ## Getting the Best RFC model 
print("Best Hyper Parameter Value For Random Forest Is: ",OptimalHyperparameters)
print("Best Precision Score By RFC Is: ",MeanCVScore)
print("Best Validated RFC Model Is: ",Best_RFC_Model)
featimp = pd.Series(Best_RFC_Model.feature_importances_, index=list(x)).sort_values(ascending=False)
print(featimp)# Getting Feature Importance List 

print()
print()

##### RFC using significant features ######  (RFC model built with the following combination of variables gave the best score as compared to all the other combinations of significant variables)
X_ = data[['Survival Months','Age','Regional Node Examined','Tumor Size','Grade','6th Stage']] ## defining the new feature set for rfc with significant variables

### Scaling the significant variables####
feature_scaler = StandardScaler()
X_scaled_ = feature_scaler.fit_transform(X_)

# Dividing the new feature set into training and test sets
X_train_, X_test_, Y_train_, Y_test_ = train_test_split( X_scaled_, y, test_size = 0.3, random_state = 100)

### RFC with significant variables 
print("################################################")
print("RFC model using significant variables")
rfc = RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1)
grid = {'n_estimators': [1,2,3,4,5,10,20,30,40,50,100]}
gridsearch = GridSearchCV(estimator=rfc, param_grid=grid, scoring='precision', cv=5)
gridsearch.fit(X_train_, Y_train_)
OptimalHyperparameters = gridsearch.best_params_
Mean_CV_Score = gridsearch.best_score_ # Mean cross-validated score of the best_estimator
Best_RFC_Model_ = gridsearch.best_estimator_
print("Best Hyper Parameter Value For RFC Is: ",OptimalHyperparameters)
print("Best Precision Score Is: ",Mean_CV_Score)
print("Best Validated Model Is: ",Best_RFC_Model_)


print()
print()


## Since RFC model without feature selection gave the best precision score, it will be implemented on the test data
#### RFC on test data ###########  
print("Evaluating the best performing RFC model on test data")
Y_Pred = Best_RFC_Model.predict(X_test) ## Making predictions of test data
accuracy = metrics.accuracy_score(Y_test, Y_Pred) ### Comparing predicted values of Y with actual values of Y 
precision = metrics.precision_score(Y_test, Y_Pred)
recall = metrics.recall_score(Y_test, Y_Pred)
f1 = metrics.f1_score(Y_test, Y_Pred) 

#### Confusion Matrix #############
conf_mat = metrics.confusion_matrix(Y_test, Y_Pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat,annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1,1])
print('TN: ', conf_mat[0,0])
print('FP: ', conf_mat[0,1])
print('FN: ', conf_mat[1,0])

print("Accuracy Score Of RFC On Test Data",accuracy)
print("Precision Score Of RFC Test Data",precision)
print("Recall Score Of RFC Test Data",recall)
print("F1 Score Of RFC Test Data",f1)

print()
print()
print(classification_report(Y_test, Y_Pred))

